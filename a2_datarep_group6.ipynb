{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcMf4aubeMI9"
   },
   "source": [
    "*****************************************************************\n",
    "#  The Social Web: data representation\n",
    "- Instructors: Jacco van Ossenbruggen, Dayana Spagnuelo\n",
    "- TAs Michael Accetto, Oktay Kavi, Abhirup Mukherjee, Nihat Uzunalioğlu\n",
    "- Exercises for Hands-on session 2\n",
    "*****************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zhts5HMzeMI-"
   },
   "source": [
    "In this session you are going to mine data in various microformats. You will see the differences in what each of the formats can contain and what purpose they serve. We will start by looking at geographical data.\n",
    "\n",
    "Prerequisites:\n",
    "- Python 3.8\n",
    "- Python packages: requests, BeautifulSoup4, HTMLParser, rdflib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "6f-OtFPPeMJA",
    "outputId": "9bcb836f-4204-4fac-d133-99e81a0b2884"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/chieh/anaconda3/lib/python3.7/site-packages (2.24.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/chieh/anaconda3/lib/python3.7/site-packages (from requests) (1.25.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/chieh/anaconda3/lib/python3.7/site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/chieh/anaconda3/lib/python3.7/site-packages (from requests) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/chieh/anaconda3/lib/python3.7/site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: BeautifulSoup4 in /Users/chieh/anaconda3/lib/python3.7/site-packages (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in /Users/chieh/anaconda3/lib/python3.7/site-packages (from BeautifulSoup4) (2.0.1)\n",
      "Collecting HTMLParser\n",
      "  Downloading HTMLParser-0.0.2.tar.gz (6.0 kB)\n",
      "Building wheels for collected packages: HTMLParser\n",
      "  Building wheel for HTMLParser (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for HTMLParser: filename=HTMLParser-0.0.2-py3-none-any.whl size=5984 sha256=4f1e489e95a38491bf4d041ce4f0f6d1d3810864a4d8f221a2110b8b56894059\n",
      "  Stored in directory: /Users/chieh/Library/Caches/pip/wheels/88/0f/43/11747d95b28379b346c15f935f4d4075e7a4ec068d3a510c79\n",
      "Successfully built HTMLParser\n",
      "Installing collected packages: HTMLParser\n",
      "Successfully installed HTMLParser-0.0.2\n",
      "Collecting rdflib\n",
      "  Downloading rdflib-5.0.0-py3-none-any.whl (231 kB)\n",
      "\u001b[K     |████████████████████████████████| 231 kB 3.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /Users/chieh/anaconda3/lib/python3.7/site-packages (from rdflib) (1.15.0)\n",
      "Collecting isodate\n",
      "  Downloading isodate-0.6.0-py2.py3-none-any.whl (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing in /Users/chieh/anaconda3/lib/python3.7/site-packages (from rdflib) (2.4.7)\n",
      "Installing collected packages: isodate, rdflib\n",
      "Successfully installed isodate-0.6.0 rdflib-5.0.0\n"
     ]
    }
   ],
   "source": [
    "# If you're using a virtualenv, make sure it's activated before running\n",
    "# this cell!\n",
    "!pip install requests\n",
    "!pip install BeautifulSoup4\n",
    "!pip install HTMLParser\n",
    "!pip install rdflib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irPnmIK4eMJd"
   },
   "source": [
    "##  Exercise 1\n",
    "\n",
    "Even if web pages do not use microformat, interesting data can often be extracted from the HML.You may use packages such as BeautifulSoup to extract arbitrary pieces of data from any HTML page.\n",
    "The example below shows how we can find the URL of first image in the infobox table of the wikipedia page on Amsterdam. Tip: compare the code below with HTML source code of the wikipedia page: the image url is in the \"src\" attribute of the \"img\" element of in the \"table\" element with class=\"infobox\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "9gpHw90keMJf",
    "outputId": "7ae1fe64-8d85-4a47-cfdf-422284954d81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//upload.wikimedia.org/wikipedia/commons/thumb/b/be/KeizersgrachtReguliersgrachtAmsterdam.jpg/270px-KeizersgrachtReguliersgrachtAmsterdam.jpg\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# This script requires you to add a url of a page with geotags to the commandline, e.g.\n",
    "# python geo.py 'http://en.wikipedia.org/wiki/Amsterdam'\n",
    "URL = 'https://en.wikipedia.org/wiki/Amsterdam'\n",
    "\n",
    "req = requests.get(URL, headers={'User-Agent' : \"Social Web Course Student\"})\n",
    "soup = BeautifulSoup(req.text)\n",
    "# print(req.text)\n",
    "image1 = soup.findAll('table', class_='infobox')[0].find('img')\n",
    "print(image1['src'])  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting coordinates from a webpage and reformatting them in the geo microformat (based on Example 8-1 in Mining the Social Web). Note that wikipages may encode long/lat information in different ways. On of the ways used by the Amsterdam wikipedia page is in a span element that is not shown to the user: \n",
    "<span class=\"geo\">52.367; 4.900</span>\n",
    "This span element has a single child: len(geoTag == 1) and no further structure, we have to manually get the long/lat by splitting the string on the ';' semicolon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "LtHtQT9PeMJl",
    "outputId": "8a7f7b52-cdb2-409f-b3f0-ee7adf60a9f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<span class=\"geo\">52.367; 4.900</span>\n",
      "Location is at 52.367 4.900\n"
     ]
    }
   ],
   "source": [
    "\n",
    "geoTag = soup.find(True, 'geo')\n",
    "print(geoTag)\n",
    "\n",
    "if geoTag and len(geoTag) > 1:\n",
    "        lat = geoTag.find(True, 'latitude').string\n",
    "        lon = geoTag.find(True, 'longitude').string\n",
    "        print ('Location is at'), lat, lon\n",
    "elif geoTag and len(geoTag) == 1:\n",
    "        (lat, lon) = geoTag.string.split(';')\n",
    "        (lat, lon) = (lat.strip(), lon.strip())\n",
    "        print (('Location is at'), lat, lon)\n",
    "else:\n",
    "        print ('Location not found')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8S_bXnjveMJp"
   },
   "source": [
    "### Task 1\n",
    "\n",
    "Can you convert the output of Exercise 1 into KML? Here is the KML documentation: https://developers.google.com/kml/documentation/?csw=1 and here you can find a simple example of how it is used: https://renenyffenegger.ch/notes/tools/Google-Earth/kml/index\n",
    "\n",
    "Visualise the point in Google Maps using the following code example: https://developers.google.com/maps/documentation/javascript/examples/layer-kml-features\n",
    "You will have to create your own KML file for the custom map layer, and provide a URL to the KML file inside the JavaScript code, which means that you have to upload the file somewhere. You can use a service like http://pastebin.com/ to obtain a URL for your KML file —> paste the code there and request the RAW format URL; use this one in this Task1.\n",
    "\n",
    "Is KML a microformat, why (not)?\n",
    "\n",
    "## Ans: \n",
    "We convert the coordinate obtained from the Exercise into KML at the link of: https://pastebin.com/raw/q2Vje0zU\n",
    "And then visualize it in https://jsfiddle.net/dvzfbxkh/2/\n",
    "We think that the KML is a microformat since it provides some information like placemark and coordinate. This kind of information can be readible by users and be procssible by softweare. Therefore, we think KML is a microformat.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUnka7EyeMJp"
   },
   "source": [
    "## Exercise 2 \n",
    "In order to find information in the web we can use microformats. However in this example you will not be using hRecipe. Instead, we'll show you how to find arbitrary tags in a webpage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0pBs-PVeMJq"
   },
   "source": [
    "### Task 2 \n",
    "Parsing data for a <sub><sup>veggie</sup></sub> spaghetti alla carbonara recipe (from Example 2-7 in Mining the Social Web)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mt9BK_CZeMJr"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# A yummy webpage (feel free to change to your likings.)\n",
    "URL = \"https://www.acouplecooks.com/spring-vegetarian-spaghetti-carbonara/\"\n",
    "\n",
    "# requests will return the html found at the given webpage...\n",
    "page = requests.get(URL)\n",
    "# ...and a BeautifulSoup object can be created from its content.\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "listchildren = list(soup.children)\n",
    "#print(listchildren)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IhdMwqykeMJt"
   },
   "source": [
    "We can find any element in the page through *css tag selectors*\n",
    "You can find them all [here](https://www.w3schools.com/cssref/css_selectors.asp), but shortly these are \".\" for classes, # for ids and plain text for the element name.\n",
    "\n",
    "\n",
    "You can also combine them, so that looking for \".class1.class2\" would select all elements displaying both classes. For a deeper overview please check the above link (or google \"html tag selectors\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "PBaiK8OLeMJu",
    "outputId": "5b75f973-41c1-4ad4-fd9f-4f1f7665ba1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[<li><span data-amount=\"1\">1</span> pound spaghetti noodles</li>, <li><span data-amount=\"0.5\" data-unit=\"cup\">½ cup</span> smoked mozzarella cheese</li>, <li><span data-amount=\"0.5\" data-unit=\"cup\">½ cup</span> grated Parmesan cheese, plus more for serving</li>, <li><span data-amount=\"4\">4</span> egg yolks</li>, <li><span data-amount=\"1\" data-unit=\"cup\">1 cup</span> frozen Earthbound Farm Organic peas</li>, <li><span data-amount=\"8\" data-unit=\"cup\">8 cups</span> Earthbound Farm Organic spinach</li>, <li><span data-amount=\"3\" data-unit=\"tablespoon\">3 tablespoons</span> butter</li>, <li><a class=\"tasty-link\" data-tasty-links-no-disclosure=\"\" href=\"https://www.acouplecooks.com/what-is-kosher-salt/\" target=\"_blank\">Kosher salt</a></li>, <li>Fresh ground black pepper</li>]\n"
     ]
    }
   ],
   "source": [
    "print(len(listchildren)) # we can see here how many children the html doc has got.\n",
    "ingredients_unparsed = soup.select_one(\".tasty-recipes-ingredients\")\n",
    "# let's get all the \"list item\" elements in a list:\n",
    "ing_unp = ingredients_unparsed.findAll('li')\n",
    "print(ing_unp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFXVPZhIeMJw"
   },
   "source": [
    "Mmmh... not so pretty yet. How about listing their items using the text method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "xASBZsnMeMJx",
    "outputId": "7af0f6e9-3b4f-4f34-e444-794087d06e25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingredients:\n",
      "\n",
      "1 pound spaghetti noodles\n",
      "½ cup smoked mozzarella cheese\n",
      "½ cup grated Parmesan cheese, plus more for serving\n",
      "4 egg yolks\n",
      "1 cup frozen Earthbound Farm Organic peas\n",
      "8 cups Earthbound Farm Organic spinach\n",
      "3 tablespoons butter\n",
      "Kosher salt\n",
      "Fresh ground black pepper\n"
     ]
    }
   ],
   "source": [
    "ingredients = [t.text for t in ing_unp]\n",
    "print(\"Ingredients:\\n\")\n",
    "# [print(i) for i in ingredients]  # Also prints the generator\n",
    "# Instead\n",
    "for ing in ingredients:\n",
    "    print(ing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-RItVHyeMJz"
   },
   "source": [
    "Good. Now the instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "d-3Op4B6eMJ0",
    "outputId": "75a70f0c-86d3-4be9-d2d8-84df91c4f392"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instructions:\n",
      "\n",
      "In a large pot, combine 6 quarts of water with 2 tablespoons kosher salt and bring it to a boil.\n",
      "Grate the Parmesan and mozzarella cheese. Carefully separate four egg yolks and set aside.\n",
      "Once boiling, add the pasta and cook until the pasta is just about al dente, about 7 minutes; then add peas and spinach and cook for 1 minute. Reserve 1 cup cooking water, and then drain the pasta and vegetables.\n",
      "In a skillet, melt the butter, then stir in the cheeses, ¼ cup pasta water, and ¼ teaspoon kosher salt. Stir in the pasta and vegetables until creamy over low heat, adding more pasta water if necessary (note that the mozzarella will stick together in some places).\n",
      "To serve, top each pasta serving with a whole egg yolk and additional Parmesan cheese, and stir the yolk into the pasta at the table (if you are uncomfortable serving egg yolks at the table, stir the egg yolks into the pasta in the skillet to heat them through). Serve immediately. (Note that the mozzarella cheese can become gummy the longer the pasta sits, so eat immediately if possible. Leftovers can be reheated in a skillet, but may not have the same creamy texture.)\n"
     ]
    }
   ],
   "source": [
    "instructions_unparsed = soup.select_one(\".tasty-recipes-instructions\")\n",
    "instructions_unparsed = instructions_unparsed.findAll(\"li\")\n",
    "#print(instructions_unparsed)\n",
    "instructions = [inst.text for inst in instructions_unparsed]\n",
    "print(\"Instructions:\\n\")\n",
    "for ins in instructions:\n",
    "    print(ins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPWXuglfeMJ2"
   },
   "source": [
    "Let's finish off with the title:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "yg1TnWe2eMJ3",
    "outputId": "05d39a2e-3779-45f1-ddeb-9c6d2ae5f494"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n\\n\\nVegetarian Carbonara\\n\\nRecipes ', ' Fast Dinner Ideas ', ' Vegetarian Carbonara\\n']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Vegetarian Carbonara'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_unparsed = soup.select_one(\".post-header\") # \n",
    "categorical_title = title_unparsed.text.split(\"›\") # website specific divider.\n",
    "print(categorical_title)\n",
    "recipe_title = categorical_title[-1].strip() # let's remove that ugly space at the beginning.\n",
    "recipe_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYb6WtXYeMJ6"
   },
   "source": [
    "## Task 2.1\n",
    "Now it's your turn. Create a function that can scrape any recipe webpage from the same website (other websites will have different class tags). \n",
    "\n",
    "Make sure to:\n",
    "\n",
    "- return itemized content (e.g. ingredients) in a list. You may want to use a list comprehension here.\n",
    "- Not all items have been cleaned of their html markdown (see variables ```ingredients``` vs. ```instructions_unparsed```. Make sure to return a list with human readable content (i.e. by using the ```.text``` attribute).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "UQu9ecLEeMJ6",
    "outputId": "a8aa0e14-a8fb-4279-cf32-8dca97ab3412"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"name\": \"Best Blueberry Crisp Recipe\",\n",
      "    \"ingredients\": [\n",
      "        \"5 cups fresh blueberries\",\n",
      "        \"1 tablespoon vanilla extract\",\n",
      "        \"2 tablespoons fresh lemon juice\",\n",
      "        \"2 tablespoons arrowroot powder or cornstarch\",\n",
      "        \"1/4 teaspoon kosher salt\",\n",
      "        \"5 tablespoons cold unsalted butter\",\n",
      "        \"1 cup rolled oats\",\n",
      "        \"1/2 cup coconut sugar or granulated sugar\",\n",
      "        \"1/2 cup almond flour\",\n",
      "        \"1 teaspoon vanilla extract\",\n",
      "        \"1/2 teaspoon kosher salt\",\n",
      "        \"1 teaspoon culinary lavender, crushed under the bottom of a glass until powdery (optional)*\"\n",
      "    ],\n",
      "    \"instructions\": [\n",
      "        \"Preheat the oven to 350F. Lightly grease an 8-inch pie plate or a cast-iron skillet.\",\n",
      "        \"Make the filling: In a large bowl, toss together the blueberries, vanilla, lemon juice, arrowroot and salt until well coated. Transfer the filling to the prepared pie plate.\",\n",
      "        \"Make the topping: Wipe out the bowl, then chop the butter into small pieces and add it to the bowl, with the oats, coconut sugar, almond flour, vanilla, salt, and lavender. Pinch it all together with your fingertips until it forms a shaggy, sandy dough. Sprinkle the topping evenly over the filling.\",\n",
      "        \"Bake for about 45 minutes, or until the filling bubbles and the topping is golden brown. Allow to cool at least 15 minutes.\\u00a0\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Pass in a URL containing hRecipe, such as\n",
    "# https://www.jamieoliver.com/recipes/pasta-recipes/veggie-carbonara/\n",
    "\n",
    "URL = \"https://www.acouplecooks.com/easy-blueberry-crisp/\"#YOUR RECIPE HERE/\n",
    "\n",
    "# Parse out some of the pertinent information for a recipe.\n",
    "# See http://microformats.org/wiki/hrecipe.\n",
    "\n",
    "def parse_website(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    # You code here\n",
    "    # Parse header and get the title\n",
    "    title_unparsed = soup.select_one(\".post-header\") # \n",
    "    categorical_title = title_unparsed.text.split(\"›\") # website specific divider.\n",
    "    recipe_title = categorical_title[-1].strip() # let's remove that ugly space at the beginning.\n",
    "    fn = recipe_title\n",
    "    #print(fn)\n",
    "\n",
    "    # Ingredients\n",
    "    ingredients_unparsed = soup.select_one(\".tasty-recipes-ingredients\")\n",
    "    # let's get all the \"list item\" elements in a list:\n",
    "    ing_unp = ingredients_unparsed.findAll('li')\n",
    "    ingredients = [t.text for t in ing_unp]\n",
    "\n",
    "    # Instructions\n",
    "    instructions_unparsed = soup.select_one(\".tasty-recipes-instructions\")\n",
    "    instructions_unparsed = instructions_unparsed.findAll(\"li\")\n",
    "    instructions = [t.text for t in instructions_unparsed]\n",
    "\n",
    "    return {\n",
    "            'name': fn,\n",
    "            'ingredients': ingredients,\n",
    "            'instructions': instructions,\n",
    "            }\n",
    "    \n",
    "recipe = parse_website(URL)\n",
    "#print (recipe)\n",
    "print(json.dumps(recipe,indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccURluAIeMJ8"
   },
   "source": [
    "But How can we get information not only from one website,  but from all? \n",
    "\n",
    "The answer: microformats.\n",
    "\n",
    "But rather than extracting with information manually from the schema.org or hRecipe microformats, we can use a package, ```scrape-schema-recipe``` \n",
    "\n",
    "Feel free to experiment with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scrape_schema_recipe in c:\\socialenv\\lib\\site-packages (0.1.3)\n",
      "Requirement already satisfied: extruct in c:\\socialenv\\lib\\site-packages (from scrape_schema_recipe) (0.10.0)\n",
      "Requirement already satisfied: isodate>=0.5.1 in c:\\socialenv\\lib\\site-packages (from scrape_schema_recipe) (0.6.0)\n",
      "Requirement already satisfied: validators>=0.12.4 in c:\\socialenv\\lib\\site-packages (from scrape_schema_recipe) (0.18.1)\n",
      "Requirement already satisfied: requests in c:\\socialenv\\lib\\site-packages (from scrape_schema_recipe) (2.24.0)\n",
      "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in c:\\socialenv\\lib\\site-packages (from scrape_schema_recipe) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=39.2.0 in c:\\socialenv\\lib\\site-packages (from scrape_schema_recipe) (49.2.1)\n",
      "Requirement already satisfied: rdflib-jsonld in c:\\socialenv\\lib\\site-packages (from extruct->scrape_schema_recipe) (0.5.0)\n",
      "Requirement already satisfied: mf2py in c:\\socialenv\\lib\\site-packages (from extruct->scrape_schema_recipe) (1.1.2)\n",
      "Requirement already satisfied: jstyleson in c:\\socialenv\\lib\\site-packages (from extruct->scrape_schema_recipe) (0.0.2)\n",
      "Requirement already satisfied: html-text>=0.5.1 in c:\\socialenv\\lib\\site-packages (from extruct->scrape_schema_recipe) (0.5.2)\n",
      "Requirement already satisfied: lxml in c:\\socialenv\\lib\\site-packages (from extruct->scrape_schema_recipe) (4.6.1)\n",
      "Requirement already satisfied: w3lib in c:\\socialenv\\lib\\site-packages (from extruct->scrape_schema_recipe) (1.22.0)\n",
      "Requirement already satisfied: six in c:\\socialenv\\lib\\site-packages (from extruct->scrape_schema_recipe) (1.15.0)\n",
      "Requirement already satisfied: rdflib<5.0.0 in c:\\socialenv\\lib\\site-packages (from extruct->scrape_schema_recipe) (4.2.2)\n",
      "Requirement already satisfied: decorator>=3.4.0 in c:\\socialenv\\lib\\site-packages (from validators>=0.12.4->scrape_schema_recipe) (4.4.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\socialenv\\lib\\site-packages (from requests->scrape_schema_recipe) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\socialenv\\lib\\site-packages (from requests->scrape_schema_recipe) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\socialenv\\lib\\site-packages (from requests->scrape_schema_recipe) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\socialenv\\lib\\site-packages (from requests->scrape_schema_recipe) (1.25.11)\n",
      "Requirement already satisfied: html5lib>=1.0.1 in c:\\socialenv\\lib\\site-packages (from mf2py->extruct->scrape_schema_recipe) (1.1)\n",
      "Requirement already satisfied: BeautifulSoup4>=4.6.0 in c:\\socialenv\\lib\\site-packages (from mf2py->extruct->scrape_schema_recipe) (4.9.3)\n",
      "Requirement already satisfied: pyparsing in c:\\socialenv\\lib\\site-packages (from rdflib<5.0.0->extruct->scrape_schema_recipe) (2.4.7)\n",
      "Requirement already satisfied: webencodings in c:\\socialenv\\lib\\site-packages (from html5lib>=1.0.1->mf2py->extruct->scrape_schema_recipe) (0.5.1)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in c:\\socialenv\\lib\\site-packages (from BeautifulSoup4>=4.6.0->mf2py->extruct->scrape_schema_recipe) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "#try scrape-schema-recipe\n",
    "!pip install scrape_schema_recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "dict_keys(['@context', '@type', 'name', 'description', 'author', 'keywords', 'image', 'url', 'recipeIngredient', 'recipeInstructions', 'prepTime', 'cookTime', 'totalTime', 'recipeYield', 'recipeCategory', 'cookingMethod', 'recipeCuisine', 'aggregateRating', 'nutrition', 'datePublished', '@id', 'isPartOf', 'mainEntityOfPage'])\n",
      "Best Blueberry Crisp Recipe\n",
      "\n",
      " Published date:\n",
      "2019-04-23\n",
      "\n",
      " Ingredients:\n",
      "5 cups fresh blueberries\n",
      "1 tablespoon vanilla extract\n",
      "2 tablespoons fresh lemon juice\n",
      "2 tablespoons arrowroot powder or cornstarch\n",
      "1/4 teaspoon kosher salt\n",
      "5 tablespoons cold unsalted butter\n",
      "1 cup rolled oats\n",
      "1/2 cup coconut sugar or granulated sugar\n",
      "1/2 cup almond flour\n",
      "1 teaspoon vanilla extract\n",
      "1/2 teaspoon kosher salt\n",
      "1 teaspoon culinary lavender, crushed under the bottom of a glass until powdery (optional)*\n",
      "\n",
      " Instructions:\n",
      "[\n",
      "    {\n",
      "        \"@type\": \"HowToStep\",\n",
      "        \"text\": \"Preheat the oven to 350F. Lightly grease an 8-inch pie plate or a cast-iron skillet.\",\n",
      "        \"url\": \"https://www.acouplecooks.com/easy-blueberry-crisp/#instruction-step-1\"\n",
      "    },\n",
      "    {\n",
      "        \"@type\": \"HowToStep\",\n",
      "        \"text\": \"Make the filling: In a large bowl, toss together the blueberries, vanilla, lemon juice, arrowroot and salt until well coated. Transfer the filling to the prepared pie plate.\",\n",
      "        \"url\": \"https://www.acouplecooks.com/easy-blueberry-crisp/#instruction-step-2\"\n",
      "    },\n",
      "    {\n",
      "        \"@type\": \"HowToStep\",\n",
      "        \"text\": \"Make the topping: Wipe out the bowl, then chop the butter into small pieces and add it to the bowl, with the oats, coconut sugar, almond flour, vanilla, salt, and lavender. Pinch it all together with your fingertips until it forms a shaggy, sandy dough. Sprinkle the topping evenly over the filling.\",\n",
      "        \"url\": \"https://www.acouplecooks.com/easy-blueberry-crisp/#instruction-step-3\"\n",
      "    },\n",
      "    {\n",
      "        \"@type\": \"HowToStep\",\n",
      "        \"text\": \"Bake for about 45 minutes, or until the filling bubbles and the topping is golden brown. Allow to cool at least 15 minutes.\\u00a0\",\n",
      "        \"url\": \"https://www.acouplecooks.com/easy-blueberry-crisp/#instruction-step-4\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import scrape_schema_recipe\n",
    "URL = \"https://www.acouplecooks.com/easy-blueberry-crisp/\"\n",
    "recipe_new = scrape_schema_recipe.scrape_url(URL, python_objects=True)\n",
    "print(len(recipe_new))\n",
    "\n",
    "recipe_new = recipe_new[0]\n",
    "#keywords\n",
    "print()\n",
    "print(recipe_new.keys())\n",
    "\n",
    "#print recipe name\n",
    "print(recipe_new['name'])\n",
    "\n",
    "#print recipe published date\n",
    "print('\\n Published date:')\n",
    "print(recipe_new['datePublished'])\n",
    "\n",
    "#print recipe ingredient\n",
    "ingredients_new = recipe_new['recipeIngredient']\n",
    "print(\"\\n Ingredients:\")\n",
    "for ing in ingredients_new:\n",
    "    print(ing)\n",
    "\n",
    "#print instructions\n",
    "print(\"\\n Instructions:\")\n",
    "instructions_new = recipe_new['recipeInstructions']\n",
    "print(json.dumps(instructions_new, indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBY-y_GreMJ8"
   },
   "source": [
    "### Task 2.2\n",
    "hRecipe is a microformat specifically created for recipes.\n",
    "Can you for example easily compare different dessert recipe ingredients? For inspiration you can look back at the exercises you did in Hands-on session 1 where you compared different sets of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingredients of acouplecooks recipe:\n",
      "\n",
      "1 1/4 cups plus 1 teaspoon all-purpose flour, divided\n",
      "2/3 cup firmly packed cup brown sugar\n",
      "1 teaspoon ground cinnamon\n",
      "3/4 teaspoon baking powder\n",
      "3/4 teaspoon baking soda\n",
      "1/4 teaspoon kosher salt\n",
      "1 egg\n",
      "1/3 cup olive oil\n",
      "1/2 cup plain Greek yogurt\n",
      "1/2 cup unsweetened applesauce\n",
      "1 teaspoon vanilla extract\n",
      "1 1/2 cups blueberries\n",
      "1 teaspoon lemon zest plus 1 teaspoon lemon juice\n",
      "Powdered sugar (optional), for dusting\n",
      "-----------------------------------------\n",
      "Ingredients of jamieoliver recipe:\n",
      "\n",
      "150 g unsalted butter (at room temperature), plus extra for greasing\n",
      "200 g caster sugar \n",
      "1  lemon \n",
      "150 g fine cornmeal \n",
      "150 g ground almonds \n",
      "½ teaspoon baking powder \n",
      "4 large free-range eggs \n",
      "2 teaspoons vanilla extract \n",
      "200 g blueberries \n"
     ]
    }
   ],
   "source": [
    "re01_url = \"https://www.acouplecooks.com/blueberry-cake/\"\n",
    "re02_url = \"https://www.jamieoliver.com/recipes/fruit-recipes/blueberry-cornmeal-skillet-cake/\"\n",
    "\n",
    "recipe_acouplecooks = scrape_schema_recipe.scrape_url(re01_url, python_objects=True)\n",
    "recipe_jamie = scrape_schema_recipe.scrape_url(re02_url, python_objects=True)\n",
    "\n",
    "recipe_acouplecooks = recipe_acouplecooks[0]\n",
    "recipe_jamie = recipe_jamie[0]\n",
    "\n",
    "#acouplecooks ingredients\n",
    "ingredients_acouplecooks = recipe_acouplecooks['recipeIngredient']\n",
    "print(\"Ingredients of acouplecooks recipe:\\n\")\n",
    "for ing in ingredients_acouplecooks:\n",
    "    print(ing)\n",
    "print(\"-----------------------------------------\")\n",
    "#jamie ingredients\n",
    "ingredients_jamie = recipe_jamie['recipeIngredient']\n",
    "print(\"Ingredients of jamieoliver recipe:\\n\")\n",
    "for ing in ingredients_jamie:\n",
    "    print(ing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-J8fiLbeMJ9"
   },
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7XBeqJHVeMJ9"
   },
   "source": [
    "Schema.org is one of the most widely used annotations formats. Schema.org is a multipurpose  template that has been created by a consortium consisting of Yahoo!, Google and Microsoft. It can describe entities, events, products etc. Check out the vocabulary specs on Schema.org."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fiw8JClyeMJ-"
   },
   "source": [
    "### Task 3\n",
    "\n",
    "Parsing schema.org microdata. To parse this data you need to install the rdflib-microdata package, which you have done in one of the previous steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "id": "X2zr3fOOeMJ-",
    "outputId": "d123f981-d73f-470f-b5e9-8735819f894b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://dbpedia.org/resource/Micheal_Jackson http://dbpedia.org/ontology/wikiPageRevisionID 631226997\n",
      "http://dbpedia.org/resource/Micheal_Jackson http://www.w3.org/2002/07/owl#sameAs http://dbpedia.org/resource/Micheal_Jackson\n",
      "http://dbpedia.org/resource/Micheal_Jackson http://www.w3.org/ns/prov#wasDerivedFrom http://en.wikipedia.org/wiki/Micheal_Jackson?oldid=631226997\n",
      "http://en.wikipedia.org/wiki/Micheal_Jackson http://xmlns.com/foaf/0.1/primaryTopic http://dbpedia.org/resource/Micheal_Jackson\n",
      "http://dbpedia.org/resource/Micheal_Jackson http://dbpedia.org/ontology/wikiPageRedirects http://dbpedia.org/resource/Michael_Jackson\n",
      "http://dbpedia.org/resource/Micheal_Jackson http://www.w3.org/2000/01/rdf-schema#label Micheal Jackson\n",
      "http://dbpedia.org/resource/Micheal_Jackson http://dbpedia.org/ontology/wikiPageID 14995602\n",
      "http://dbpedia.org/resource/Micheal_Jackson http://xmlns.com/foaf/0.1/isPrimaryTopicOf http://en.wikipedia.org/wiki/Micheal_Jackson\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph\n",
    "\n",
    "# Source: https://www.youtube.com/watch?v=sCU214rbRZ0\n",
    "# Pass in a URL containing Schema.org microformats\n",
    "URL = \"http://dbpedia.org/resource/Micheal_Jackson\"\n",
    "\n",
    "# Initialize a graph\n",
    "g = Graph()\n",
    "\n",
    "# Parse in an RDF file graph dbpedia\n",
    "result = g.parse(location=URL)\n",
    "\n",
    "# Loop through first 10 triples in the graph\n",
    "for index, (sub, pred, obj) in enumerate(g):\n",
    "    print(sub, pred, obj)\n",
    "    if index == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "hrQ2EuY5JAn1",
    "outputId": "eba60ebb-7ac5-4451-c16e-3f68e66af7f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph has 8 facts\n"
     ]
    }
   ],
   "source": [
    "# Print the size of the Graph\n",
    "print(f'Graph has {len(g)} facts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "id": "IAO1JllwJMqO",
    "outputId": "08f5e32d-d1a6-4a30-878a-ce7b768a8811"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@prefix dbo: <http://dbpedia.org/ontology/> .\n",
      "@prefix foaf: <http://xmlns.com/foaf/0.1/> .\n",
      "@prefix owl: <http://www.w3.org/2002/07/owl#> .\n",
      "@prefix prov: <http://www.w3.org/ns/prov#> .\n",
      "@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
      "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
      "@prefix xml: <http://www.w3.org/XML/1998/namespace> .\n",
      "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
      "\n",
      "<http://en.wikipedia.org/wiki/Micheal_Jackson> foaf:primaryTopic <http://dbpedia.org/resource/Micheal_Jackson> .\n",
      "\n",
      "<http://dbpedia.org/resource/Micheal_Jackson> rdfs:label \"Micheal Jackson\"@en ;\n",
      "    dbo:wikiPageID 14995602 ;\n",
      "    dbo:wikiPageRedirects <http://dbpedia.org/resource/Michael_Jackson> ;\n",
      "    dbo:wikiPageRevisionID 631226997 ;\n",
      "    owl:sameAs <http://dbpedia.org/resource/Micheal_Jackson> ;\n",
      "    prov:wasDerivedFrom <http://en.wikipedia.org/wiki/Micheal_Jackson?oldid=631226997> ;\n",
      "    foaf:isPrimaryTopicOf <http://en.wikipedia.org/wiki/Micheal_Jackson> .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print out the entire Graph in the RDF Turtle format\n",
    "print(g.serialize(format='ttl').decode('u8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzbynasAeMKA"
   },
   "source": [
    "### Task 3.1 \n",
    "Compare the schema.org information about a band on last.fm to the Facebook Open Graph information about the same band from Facebook. What are the differences? Which format do you think supports better interoperability?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ans:\n",
    "   The following two cells show the result of the band BTS on last.fm in schema.org information and in Facebook Open Graph information. The different between these two presentations is huge. First, the most difference is that schema.org using rdflib shows more detail and stuctured metadata of prefixes and knowledgement. While, Facebook Open Graph shows very limited contents. Second, Open Graph for Python lib provided in their index page has been no longer maintained. The handy way to get Open Graph of one web data is to use Facebook development tools, which is special for url when sharing in Facebook, not appropriate for our to represent knowledgment nor parse work. Moreover, the format of schema.org can explicit to serialize the graph, which is helpful in later processing. In our opinion, the format of schema.org is better to support interoperability and universality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph has 59 facts\n",
      "@prefix dbo: <http://dbpedia.org/ontology/> .\n",
      "@prefix foaf: <http://xmlns.com/foaf/0.1/> .\n",
      "@prefix owl: <http://www.w3.org/2002/07/owl#> .\n",
      "@prefix prov: <http://www.w3.org/ns/prov#> .\n",
      "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
      "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
      "\n",
      "<http://dbpedia.org/resource/BTS_(disambiguation)> dbo:wikiPageRedirects <http://dbpedia.org/resource/BTS> .\n",
      "\n",
      "<http://dbpedia.org/resource/BtS> dbo:wikiPageRedirects <http://dbpedia.org/resource/BTS> .\n",
      "\n",
      "<http://en.wikipedia.org/wiki/BTS> foaf:primaryTopic <http://dbpedia.org/resource/BTS> .\n",
      "\n",
      "<http://dbpedia.org/resource/BTS> a <http://dbpedia.org/class/yago/Abstraction100002137>,\n",
      "        <http://dbpedia.org/class/yago/Company108058098>,\n",
      "        <http://dbpedia.org/class/yago/Group100031264>,\n",
      "        <http://dbpedia.org/class/yago/Institution108053576>,\n",
      "        <http://dbpedia.org/class/yago/Organization108008335>,\n",
      "        <http://dbpedia.org/class/yago/SocialGroup107950920>,\n",
      "        <http://dbpedia.org/class/yago/WikicatTelecommunicationsCompanies>,\n",
      "        <http://dbpedia.org/class/yago/YagoLegalActor>,\n",
      "        <http://dbpedia.org/class/yago/YagoLegalActorGeo>,\n",
      "        <http://dbpedia.org/class/yago/YagoPermanentlyLocatedEntity> ;\n",
      "    rdfs:label \"BTS\"@de,\n",
      "        \"BTS\"@en,\n",
      "        \"BTS\"@es,\n",
      "        \"BTS\"@fr,\n",
      "        \"BTS\"@it,\n",
      "        \"BTS\"@ja,\n",
      "        \"BTS\"@pl,\n",
      "        \"BTS\"@zh ;\n",
      "    dbo:wikiPageDisambiguates <http://dbpedia.org/resource/BTS_(band)>,\n",
      "        <http://dbpedia.org/resource/BTS_Group>,\n",
      "        <http://dbpedia.org/resource/BTS_Skytrain>,\n",
      "        <http://dbpedia.org/resource/Back_to_school_(marketing)>,\n",
      "        <http://dbpedia.org/resource/Bandar_Tasik_Selatan_station>,\n",
      "        <http://dbpedia.org/resource/Baptist_Theological_Seminary>,\n",
      "        <http://dbpedia.org/resource/Base_transceiver_station>,\n",
      "        <http://dbpedia.org/resource/Behind_the_Scenes_(disambiguation)>,\n",
      "        <http://dbpedia.org/resource/Beneath_the_Sky>,\n",
      "        <http://dbpedia.org/resource/Berjaya_Times_Square>,\n",
      "        <http://dbpedia.org/resource/Between_the_Species>,\n",
      "        <http://dbpedia.org/resource/Beyond_the_Supernatural>,\n",
      "        <http://dbpedia.org/resource/Biomedical_Tissue_Services>,\n",
      "        <http://dbpedia.org/resource/Birla_Technical_Services>,\n",
      "        <http://dbpedia.org/resource/Brevet_de_Technicien_Supérieur>,\n",
      "        <http://dbpedia.org/resource/British_Thoracic_Society>,\n",
      "        <http://dbpedia.org/resource/British_Tunnelling_Society>,\n",
      "        <http://dbpedia.org/resource/Broadcast_Television_Systems_Inc.>,\n",
      "        <http://dbpedia.org/resource/Bug_tracking_system>,\n",
      "        <http://dbpedia.org/resource/Build_to_stock>,\n",
      "        <http://dbpedia.org/resource/Built_to_Spill>,\n",
      "        <http://dbpedia.org/resource/Bureau_of_Transportation_Statistics>,\n",
      "        <http://dbpedia.org/resource/Civilization_IV:_Beyond_the_Sword> ;\n",
      "    dbo:wikiPageID 608026 ;\n",
      "    dbo:wikiPageRevisionID 745074567 ;\n",
      "    owl:sameAs <http://dbpedia.org/resource/BTS>,\n",
      "        <http://de.dbpedia.org/resource/BTS>,\n",
      "        <http://es.dbpedia.org/resource/BTS>,\n",
      "        <http://fr.dbpedia.org/resource/BTS>,\n",
      "        <http://it.dbpedia.org/resource/BTS>,\n",
      "        <http://ja.dbpedia.org/resource/BTS>,\n",
      "        <http://ko.dbpedia.org/resource/BTS>,\n",
      "        <http://pl.dbpedia.org/resource/BTS>,\n",
      "        <http://wikidata.dbpedia.org/resource/Q298548>,\n",
      "        <http://www.wikidata.org/entity/Q298548>,\n",
      "        <http://yago-knowledge.org/resource/BTS> ;\n",
      "    prov:wasDerivedFrom <http://en.wikipedia.org/wiki/BTS?oldid=745074567> ;\n",
      "    foaf:isPrimaryTopicOf <http://en.wikipedia.org/wiki/BTS> .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The schema.org information about band BTS \n",
    "url = \"http://dbpedia.org/resource/BTS\"\n",
    "\n",
    "gp = Graph()\n",
    "gp.parse(url)\n",
    "\n",
    "print(f'Graph has {len(gp)} facts')\n",
    "\n",
    "print(gp.serialize(format='ttl').decode('u8'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "the Facebook Open Graph information about BTS.\n",
    "\n",
    "<meta charset=\"utf-8\" />\n",
    "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n",
    "<meta name=\"description\" content=\"Listen to music from BTS like Dynamite, Boy with Luv (feat. Halsey) & more. Find the latest tracks, albums, and images from BTS.\" data-replaceable-head-tag=\"\" />\n",
    "<meta property=\"og:title\" content=\"BTS music, videos, stats, and photos | Last.fm\" data-replaceable-head-tag=\"\" />\n",
    "<meta property=\"og:description\" content=\"Listen to music from BTS like Dynamite, Boy with Luv (feat. Halsey) & more. Find the latest tracks, albums, and images from BTS.\" data-replaceable-head-tag=\"\" />\n",
    "<meta property=\"og:type\" content=\"website\" data-replaceable-head-tag=\"\" />\n",
    "<meta property=\"og:site_name\" content=\"Last.fm\" data-replaceable-head-tag=\"\" />\n",
    "<meta property=\"og:url\" content=\"https://www.last.fm/music/BTS\" data-replaceable-head-tag=\"\" />\n",
    "<meta property=\"og:image\" content=\"https://lastfm.freetls.fastly.net/i/u/ar0/c86b5eb66796245e7de90fb30f26f4eb.jpg\" data-replaceable-head-tag=\"\" />\n",
    "<meta property=\"og:image:width\" content=\"1200\" data-replaceable-head-tag=\"\" />\n",
    "<meta property=\"og:image:height\" content=\"630\" data-replaceable-head-tag=\"\" />\n",
    "<meta name=\"twitter:title\" content=\"BTS music, videos, stats, and photos | Last.fm\" data-replaceable-head-tag=\"\" />\n",
    "<meta name=\"twitter:description\" content=\"Listen to music from BTS like Dynamite, Boy with Luv (feat. Halsey) & more. Find the latest tracks, albums, and images from BTS.\" data-replaceable-head-tag=\"\" />\n",
    "<meta name=\"twitter:image\" content=\"https://lastfm.freetls.fastly.net/i/u/ar0/c86b5eb66796245e7de90fb30f26f4eb.jpg\" data-replaceable-head-tag=\"\" />\n",
    "<meta name=\"twitter:card\" content=\"summary\" data-replaceable-head-tag=\"\" />\n",
    "<meta name=\"twitter:site\" content=\"@lastfm\" data-replaceable-head-tag=\"\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nocs4YDPeMKB"
   },
   "source": [
    "### Task 3.2\n",
    "Explore the various microformats at http://microformats.org/ and compare the output of the exercises with the output of http://microformats.org/. Think about possible microformats you want to support in your final assignment and read up on how to parse them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ans:\n",
    "   We explore three types of microformats hCard, hCalendar, and hReview. After comparison, we find that the format of schema.org can give us a serialized output, while these three microfomats cannot. And for different microformats have different descripted objects. It is not universial for web data on many categories. To our satisfication, the hCard has the most identifiers which can better represent entities like person, orgnization, and so on. However, we suggest that we may possibly use the format of schema.org in our final assignmnet for reasons. There are some handy resources and instructions on it to help us to represent data from web and parse them."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Hands-on_2_microformats.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
